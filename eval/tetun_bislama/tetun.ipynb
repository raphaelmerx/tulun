{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from typing import List, Literal, Optional\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "from sacrebleu import corpus_chrf\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from translator_utils import Translator, Glossary, Line, LineManager, Message\n",
    "from gemini_aistudio import GenerativeModel\n",
    "\n",
    "\n",
    "MODE: Literal[\"translate\", \"post-edit\"] = \"post-edit\"\n",
    "ZERO_SHOT: bool = False\n",
    "\n",
    "if MODE == \"post-edit\":\n",
    "    # gemini = GenerativeModel(system_instruction=\"You are a linguist helping to post-edit translations from English to Tetun. Candidate translations are provided by Google Translate, and you are asked to correct them, if necessary, using examples and glossary entries. Only use examples and glossary entries to correct the translations.\")\n",
    "    system_instruction = \"You are an expert translator. I am going to give you relevant glossary entries, and relevant past translations, where the first is the English source, the second is a machine translation of the English to Tetun, and the third is the Tetun reference translation. The sentences will be written English: <sentence> MT: <machine translated sentence> Tetun: <translated sentence>. After the example pairs, I am going to provide another sentence in English and its machine translation, and I want you to translate it into Tetun. Give only the translation, and no extra commentary, formatting, or chattiness. Translate the text from English to Tetun.\"\n",
    "\n",
    "elif MODE == \"translate\":\n",
    "    if ZERO_SHOT:\n",
    "        system_instruction = \"You are an expert translator. I am going to give you text in English, and would like you to translate it to Tetun. Give only the translation, and no extra commentary, formatting, or chattiness. Translate the text from English to Tetun.\"\n",
    "    else:\n",
    "        system_instruction = \"You are an expert translator. I am going to give you some example pairs of text snippets where the first is in English and the second is a translation of the first snippet into Tetun. The sentences will be written English: <first sentence> Tetun: <translated first sentence> After the example pairs, I am going to provide another sentence in English and I want you to translate it into Tetun. Give only the translation, and no extra commentary, formatting, or chattiness. Translate the text from English to Tetun.\"\n",
    "\n",
    "gemini = GenerativeModel(system_instruction=system_instruction)\n",
    "\n",
    "TRANSLATE_WITH: Literal[\"google\", \"madlad\", \"opusmt\"] = \"madlad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "line_manager = LineManager.load_from_csv()\n",
    "train_lines, test_lines = (line_manager.train_lines, line_manager.test_lines)\n",
    "# train_lines = LineManager.load_from_csv(filename='datafiles/tetun_parallel.csv').lines\n",
    "# test_lines = LineManager.load_from_csv(filename='datafiles/parallel_lines_A2.csv').lines\n",
    "print(f\"Total of {len(train_lines)} train lines and {len(test_lines)} test lines loaded.\")\n",
    "\n",
    "# Initialize translator with BM25\n",
    "translator = Translator(translate_with=TRANSLATE_WITH)\n",
    "translator.init_bm25(train_lines)\n",
    "\n",
    "# Load glossary\n",
    "glossary = Glossary()\n",
    "glossary.load_entries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GT + Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TRANSLATE_WITH == \"google\":\n",
    "    for l in test_lines:\n",
    "        l.tgt_pred_google = translator.translate(l.en)\n",
    "\n",
    "if MODE == 'post-edit':\n",
    "    pred_key = f'tgt_pred_{TRANSLATE_WITH}'\n",
    "\n",
    "\n",
    "    mt_chrf = corpus_chrf(\n",
    "        [getattr(l, pred_key) for l in test_lines],\n",
    "        [[l.tgt for l in test_lines]],\n",
    "        word_order=2,\n",
    "    )\n",
    "    print(f\"CHRF for MT: {mt_chrf.score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import litellm\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "train_sample = random.sample(train_lines, 10)\n",
    "\n",
    "def format_messages_for_gemini(messages: List[Message]) -> List[dict]:\n",
    "    return [{\n",
    "        \"role\": \"user\" if message.role == \"user\" else \"model\",\n",
    "        \"parts\": [message.content]\n",
    "    } for message in messages if message.role != 'system']\n",
    "\n",
    "def get_post_edited_translation_gemini(input_text: str) -> str:\n",
    "    # get glossary entries and similar sentences\n",
    "    glossary_entries = glossary.get_entries(input_text)\n",
    "    similar_sentences = translator.get_top_similar_sentences_bm25(input_text, top_n=10)\n",
    "\n",
    "    messages = translator.construct_prompt_post_edit(\n",
    "        input_text, \n",
    "        similar_sentences,\n",
    "        glossary_entries=glossary_entries,\n",
    "    )\n",
    "\n",
    "    print(messages[0].content)\n",
    "    messages = format_messages_for_gemini(messages)\n",
    "\n",
    "    response = gemini.generate_content(\n",
    "        messages,\n",
    "    )\n",
    "    return response.strip()\n",
    "\n",
    "def get_final_translation_gemini(input_text: str) -> str:\n",
    "    messages = translator.construct_prompt_translation(\n",
    "        input_text,\n",
    "        train_sample if not ZERO_SHOT else [],\n",
    "        system_instruction=system_instruction,\n",
    "    )\n",
    "\n",
    "    response = litellm.completion(\n",
    "        model=\"gemini/gemini-2.0-flash\",\n",
    "        messages=[m.to_dict() for m in messages],\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'src: {test_lines[1].en}')\n",
    "# print(f'pred: {get_final_translation_gemini(test_lines[1].en)}')\n",
    "print(get_post_edited_translation_gemini(\"Always check burn again a couple of hours after first assessment unless burn has been dressed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "MAX_LINES = None\n",
    "\n",
    "if MODE == 'post-edit':\n",
    "    def process_line(line):\n",
    "        if not getattr(line, 'tgt_pred_post_edited', None):\n",
    "            line.tgt_pred_post_edited = get_post_edited_translation_gemini(line.en)\n",
    "        return line\n",
    "elif MODE == 'translate':\n",
    "    def process_line(line):\n",
    "        if not getattr(line, 'tgt_pred_gemini', None):\n",
    "            line.tgt_pred_gemini = get_final_translation_gemini(line.en)\n",
    "        return line\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    if MAX_LINES:\n",
    "        futures = [executor.submit(process_line, line) for line in test_lines[:MAX_LINES]]\n",
    "    else:\n",
    "        futures = [executor.submit(process_line, line) for line in test_lines]\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(test_lines)):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chrf_for_key(key: str):\n",
    "    chrf = corpus_chrf(\n",
    "        [getattr(l, key) for l in test_lines],\n",
    "        [[l.tgt for l in test_lines]],\n",
    "        word_order=2,\n",
    "    )\n",
    "    return chrf.score\n",
    "\n",
    "if MODE == 'post-edit':\n",
    "    mt_chrf = chrf_for_key(pred_key)\n",
    "    print(f\"CHRF for MT: {mt_chrf:.2f}\")\n",
    "\n",
    "    ape_chrf = chrf_for_key('tgt_pred_post_edited')\n",
    "    print(f\"CHRF for APE: {ape_chrf:.2f}\")\n",
    "elif MODE == 'translate':\n",
    "    gemini_chrf = chrf_for_key('tgt_pred_gemini')\n",
    "    print(f\"CHRF for Gemini: {gemini_chrf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
